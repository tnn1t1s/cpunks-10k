{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "19d0f047",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torchvision import transforms\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import matplotlib.pyplot as plt\n",
    "import pickle\n",
    "from matplotlib.colors import rgb2hex\n",
    "import matplotlib.image as mpimg\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f42bf158",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dir = \"../cpunks/data\"\n",
    "image_dir = \"../cpunks/images/training\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ebf36ff0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_punk(id):\n",
    "    '''\n",
    "       Returns a ndarray with loaded image\n",
    "    ''' \n",
    "    return mpimg.imread(f'''{image_dir}/punk{\"%04d\" % id}.png''')\n",
    "\n",
    "def unique_color_alpha_vectors(image_array):\n",
    "    # Reshape the image array to a 2D array where each row is a color-alpha vector\n",
    "    reshaped_array = image_array.reshape(-1, image_array.shape[2])\n",
    "\n",
    "    # Find unique rows (color-alpha vectors) in the reshaped array\n",
    "    unique_vectors = np.unique(reshaped_array, axis=0)\n",
    "\n",
    "    return unique_vectors\n",
    "\n",
    "def find_unique_vectors_across_arrays(arrays):\n",
    "    # Concatenate all arrays along the first two dimensions\n",
    "    concatenated_array = np.concatenate([arr.reshape(-1, 4) for arr in arrays], axis=0)\n",
    "\n",
    "    # Find unique rows (color vectors) in the concatenated array\n",
    "    unique_vectors = np.unique(concatenated_array, axis=0)\n",
    "\n",
    "    return unique_vectors"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d36b887",
   "metadata": {},
   "source": [
    "## load all 10k punks and find set of unique colors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "864aacb2",
   "metadata": {},
   "outputs": [],
   "source": [
    "x = [get_punk(i) for i in range(0,10000)]\n",
    "all_colors = find_unique_vectors_across_arrays(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "8451ec92",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class ColorOneHotMapper:\n",
    "    def __init__(self, unique_colors):\n",
    "        self.color_to_one_hot = {}\n",
    "        self.one_hot_to_color = {}\n",
    "\n",
    "        for idx, color in enumerate(unique_colors):\n",
    "            one_hot = np.zeros(len(unique_colors))\n",
    "            one_hot[idx] = 1\n",
    "\n",
    "            self.color_to_one_hot[tuple(color)] = one_hot\n",
    "            self.one_hot_to_color[tuple(one_hot)] = color\n",
    "\n",
    "    def get_one_hot(self, color):\n",
    "        color_tuple = tuple(color)\n",
    "        return self.color_to_one_hot.get(color_tuple, \n",
    "                                         \"Color not found\")\n",
    "\n",
    "    def get_color(self, one_hot):\n",
    "        one_hot_tuple = tuple(one_hot)\n",
    "        return self.one_hot_to_color.get(one_hot_tuple, \n",
    "                                         \"One-hot vector not found\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "1f7ccbe3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Color: [0.        0.2509804 1.        1.       ]\n",
      "One-hot encoding: [0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0.]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([0.       , 0.2509804, 1.       , 1.       ], dtype=float32)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "# Example usage\n",
    "unique_colors = all_colors\n",
    "mapper = ColorOneHotMapper(unique_colors)\n",
    "\n",
    "# Test with a color\n",
    "test_color = unique_colors[2]\n",
    "test_one_hot = mapper.get_one_hot(test_color)\n",
    "print(\"Color:\", test_color)\n",
    "print(\"One-hot encoding:\", test_one_hot)\n",
    "\n",
    "# Test with a one-hot vector\n",
    "retrieved_color = mapper.get_color(test_one_hot)\n",
    "retrieved_color"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "6bb1ebdc",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def convert_image_to_one_hot(image, mapper):\n",
    "    # Initialize an empty array for the one-hot encoded image\n",
    "    one_hot_encoded_image = np.zeros((image.shape[0], image.shape[1], len(mapper.color_to_one_hot)))\n",
    "    \n",
    "    # Iterate over each pixel and convert to one-hot encoding\n",
    "    for i in range(image.shape[0]):\n",
    "        for j in range(image.shape[1]):\n",
    "            color = image[i, j]\n",
    "            one_hot = mapper.get_one_hot(color)\n",
    "            if isinstance(one_hot, str):  # Check if color not found\n",
    "                continue  # or handle the error as needed\n",
    "            one_hot_index = np.argmax(one_hot)\n",
    "            one_hot_encoded_image[i, j, one_hot_index] = 1\n",
    "\n",
    "    return np.transpose(one_hot_encoded_image, (2, 0, 1))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "b94b5080",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(222, 24, 24)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#%%timeit\n",
    "image = get_punk(0)\n",
    "# Convert to one-hot encoded image\n",
    "one_hot_encoded_image = convert_image_to_one_hot(image, mapper)\n",
    "\n",
    "one_hot_encoded_image.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "2b3f7b1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from torch.utils.data import Dataset\n",
    "import numpy as np\n",
    "import torch\n",
    "\n",
    "class OneHotEncodedImageDataset(Dataset):\n",
    "    def __init__(self, directory, mapper):\n",
    "        self.directory = directory\n",
    "        self.mapper = mapper\n",
    "        self.image_files = [os.path.join(directory, f) for f in os.listdir(directory) if os.path.isfile(os.path.join(directory, f))]\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.image_files)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        image_path = self.image_files[idx]\n",
    "        image = mpimg.imread(image_path)\n",
    "        # Assuming get_image returns a numpy array of shape (24, 24, 4)\n",
    "        one_hot_encoded_image = convert_image_to_one_hot(image, self.mapper)\n",
    "        return torch.tensor(one_hot_encoded_image, dtype=torch.float32)\n",
    "\n",
    "def prepare_dataset(directory, mapper):\n",
    "    return OneHotEncodedImageDataset(directory, mapper)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "35fef1c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "unique_colors = all_colors\n",
    "mapper = ColorOneHotMapper(unique_colors)\n",
    "dataset_directory = image_dir  \n",
    "\n",
    "dataset = prepare_dataset(dataset_directory, mapper)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "9218f987",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(__main__.OneHotEncodedImageDataset, 10000, torch.Size([222, 24, 24]))"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(type(dataset),\n",
    " len(dataset),\n",
    " dataset[0].shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47da5508",
   "metadata": {},
   "source": [
    "### prepare a dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "2366d73d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "\n",
    "# Assuming 'dataset' is your instance of OneHotEncodedImageDataset\n",
    "batch_size = 32  # You can adjust the batch size as needed\n",
    "shuffle = True  # Set to True to shuffle the dataset at every epoch\n",
    "num_workers = 4  # Adjust based on your system's capabilities\n",
    "\n",
    "dataloader = DataLoader(dataset, \n",
    "                        batch_size=batch_size, \n",
    "                        shuffle=shuffle, \n",
    "                        num_workers=0)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1881a7ad",
   "metadata": {},
   "source": [
    "### Define the autoencoder model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "b434787b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "\n",
    "class OneHotAutoencoder(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(OneHotAutoencoder, self).__init__()\n",
    "        # Encoder layers\n",
    "        self.encoder = nn.Sequential(\n",
    "            nn.Linear(24*24*222, 128),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(128, 64)\n",
    "        )\n",
    "        # Decoder layers\n",
    "        self.decoder = nn.Sequential(\n",
    "            nn.Linear(64, 128),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(128, 24*24*222),\n",
    "            nn.Sigmoid()  # Use sigmoid to ensure output is between 0 and 1\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x.view(x.size(0), -1)  # Flatten the image\n",
    "        x = self.encoder(x)\n",
    "        x = self.decoder(x)\n",
    "        x = x.view(x.size(0), 24, 24, 222)  # Reshape back to the original shape\n",
    "        return x\n",
    "    \n",
    "    def encode(self, x):\n",
    "        x = x.view(x.size(0), -1)  # Flatten the image\n",
    "        encoded = self.encoder(x)\n",
    "        return encoded\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "acc3e597",
   "metadata": {},
   "source": [
    "### define the convnet model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "5c6c95b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SimpleConvNet(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(SimpleConvNet, self).__init__()\n",
    "        self.k = 3\n",
    "        self.p = 1\n",
    "        \n",
    "        # Convolutional layers\n",
    "        self.conv1 = nn.Conv2d(222, 64, kernel_size=self.k, padding=self.p)\n",
    "        self.conv2 = nn.Conv2d(64, 128, kernel_size=self.k, padding=self.p)\n",
    "        \n",
    "        # Upsampling layers\n",
    "        self.upsample = nn.Upsample(scale_factor=2, mode='nearest')\n",
    "        \n",
    "        # Output layer\n",
    "        self.out_conv = nn.Conv2d(128, 222, kernel_size=self.k, padding=self.p)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Apply the first convolutional layer and activation function\n",
    "        x = F.softmax(self.conv1(x))\n",
    "        \n",
    "        # Apply the second convolutional layer and activation function\n",
    "        x = F.softmax(self.conv2(x))\n",
    "\n",
    "        # Upsample the output of the convolutional layers\n",
    "        #x = self.upsample(x)\n",
    "        #x = self.upsample(x)\n",
    "        \n",
    "        # Apply the output layer with softmax activation\n",
    "        x = F.softmax(self.out_conv(x), dim=1)\n",
    "        return x\n",
    "\n",
    "# Create the model\n",
    "model = SimpleConvNet()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68400118",
   "metadata": {},
   "source": [
    "### Simple AutoEncoder/Decoder"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6838cd3",
   "metadata": {},
   "source": [
    "this one does a good job after only 5 epochs and training was still improving"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "74f7ccbd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class ConvAutoencoder(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(ConvAutoencoder, self).__init__()\n",
    "\n",
    "        # Encoder\n",
    "        self.encoder = nn.Sequential(\n",
    "            nn.Conv2d(222, 128, kernel_size=3, stride=2, padding=1),  # Output: [128, 12, 12]\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(128, 64, kernel_size=3, stride=2, padding=1),   # Output: [64, 6, 6]\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(64, 32, kernel_size=3, stride=2, padding=1)     # Output: [32, 3, 3]\n",
    "        )\n",
    "\n",
    "        # Decoder\n",
    "        self.decoder = nn.Sequential(\n",
    "            nn.ConvTranspose2d(32, 64, kernel_size=3, stride=2, padding=1, output_padding=1),  # Output: [64, 6, 6]\n",
    "            nn.ReLU(),\n",
    "            nn.ConvTranspose2d(64, 128, kernel_size=3, stride=2, padding=1, output_padding=1), # Output: [128, 12, 12]\n",
    "            nn.ReLU(),\n",
    "            nn.ConvTranspose2d(128, 222, kernel_size=3, stride=2, padding=1, output_padding=1) # Output: [222, 24, 24]\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.encoder(x)\n",
    "        x = self.decoder(x)\n",
    "        return x\n",
    "\n",
    "    def decode(self, z):\n",
    "        # Generate images from latent space representation\n",
    "        return self.decoder(z)\n",
    "\n",
    "# Create the autoencoder model\n",
    "model = ConvAutoencoder()\n",
    "\n",
    "\n",
    "#latent_representation = torch.randn(1, 32, 3, 3)  # Example latent space representation\n",
    "#generated_image = autoencoder.decode(latent_representation)\n",
    "#print(\"Generated Image Shape:\", generated_image.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85c42f93",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print model summary (requires additional package 'torchsummary' for formatted output)\n",
    "#from torchsummary import summary\n",
    "#summary(model, input_size=(222, 24, 24))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c0b9f3f",
   "metadata": {},
   "source": [
    "### setup the training loop"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d38988aa",
   "metadata": {},
   "source": [
    "PyTorch's nn.Conv2d expects input in the format (batch_size, channels, height, width), but our dataloader used the format (batch_size, height, width, channels)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "c733c096",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def train_autoencoder(model, dataloader, epochs=5):\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        running_loss = 0.0\n",
    "        for data in dataloader:\n",
    "            #print(data.shape)\n",
    "            inputs = data\n",
    "            #labels = labels.long()\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(inputs)\n",
    "            #print(outputs.shape)\n",
    "            loss = criterion(outputs, inputs)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            running_loss += loss.item()\n",
    "        print(f\"epoch ... {running_loss}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "7f237054",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch ... 356.9522985816002\n",
      "epoch ... 116.51032209396362\n",
      "epoch ... 47.86007644981146\n",
      "epoch ... 26.313571635633707\n",
      "epoch ... 16.256859550252557\n",
      "epoch ... 10.899090509861708\n",
      "epoch ... 7.935838439501822\n",
      "epoch ... 5.581342494115233\n",
      "epoch ... 4.27959600975737\n",
      "epoch ... 3.3016623142175376\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "# Create the model instance\n",
    "#autoencoder = OneHotAutoencoder()\n",
    "#model = SimpleConvNet()\n",
    "model = ConvAutoencoder()\n",
    "\n",
    "# Assuming you have a DataLoader named 'dataloader'\n",
    "train_autoencoder(model, dataloader, epochs=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "cd72a928",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of input: torch.Size([32, 222, 24, 24])\n"
     ]
    }
   ],
   "source": [
    "# pick an image  from the dataloader for validation\n",
    "for batch in dataloader:\n",
    "    # Assuming your batch contains input data and possibly labels/targets\n",
    "    inputs = batch  # or batch['input'] depending on how your dataset returns a batch\n",
    "    print(\"Shape of input:\", inputs.shape)\n",
    "    break  # We break the loop as we only want to check the shape of one batch\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "63eda1ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_one_hot(index, length):\n",
    "    # Create a zero vector of the specified length\n",
    "    one_hot_vector = np.zeros(length)\n",
    "    \n",
    "    # Set the value at the index to 1\n",
    "    one_hot_vector[index] = 1\n",
    "    \n",
    "    return one_hot_vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "d203c797",
   "metadata": {},
   "outputs": [],
   "source": [
    "def decode_to_rgb(decoded_one_hot, mapper):\n",
    "    # Choose the color with the highest probability for each pixel\n",
    "    color_indices = np.argmax(decoded_one_hot, axis=-1)\n",
    "\n",
    "    # Initialize an empty array for the RGB image\n",
    "    rgb_image = np.zeros((color_indices.shape[0], color_indices.shape[1], 4))\n",
    "\n",
    "    # Map each index back to an RGB color\n",
    "    for i in range(color_indices.shape[0]):\n",
    "        for j in range(color_indices.shape[1]):\n",
    "            one_hot_vector = get_one_hot(color_indices[i, j], 222)\n",
    "            rgb_image[i, j] = mapper.get_color(one_hot_vector)\n",
    "\n",
    "    return rgb_image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "f6c8a53b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Pass the image through the network\n",
    "preprocessed_image = inputs\n",
    "model.eval()  # Set the model to evaluation mode\n",
    "with torch.no_grad():\n",
    "    output_image = model(preprocessed_image.squeeze(0))  # Add batch dimension\n",
    "\n",
    "output_image[0].shape\n",
    "decoded_one_hot = output_image[0]\n",
    "# Permute the tensor to change the shape to (24, 24, 222)\n",
    "permuted_tensor = decoded_one_hot.permute(1, 2, 0)\n",
    "\n",
    "# Convert the permuted tensor to a numpy array\n",
    "a = permuted_tensor.numpy()\n",
    "img = decode_to_rgb(a, mapper)\n",
    "plt.imshow(img)\n",
    "plt.axis('off')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a58310c",
   "metadata": {},
   "source": [
    "#### sample from latent space and plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "04647086",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Example usage\n",
    "latent_representation = torch.randn(1, 32, 3, 3)  # Example latent space representation\n",
    "generated_image = model.decode(latent_representation)\n",
    "generated_image.shape\n",
    "tensor_squeezed = generated_image.squeeze(0).permute(1, 2, 0)  # Now the shape is [24, 24, 222]\n",
    "\n",
    "# Convert to a NumPy array\n",
    "numpy_array = tensor_squeezed.detach().numpy()\n",
    "\n",
    "img = decode_to_rgb(numpy_array, mapper)\n",
    "plt.imshow(img)\n",
    "plt.axis('off')\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
